{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c86af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of this code is adapted from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "#The following function generates a sample from a probability distribution. You may choose to ignore the logic. Just see how to use it.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "def weighted_choice(v, p):\n",
    "   total = sum(p)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in zip(v,p):\n",
    "      if upto + w >= r:\n",
    "         return c\n",
    "      upto += w\n",
    "   assert False, \"Shouldn't get here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7056c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the above function to sample from a distribution\n",
    "\n",
    "#6-faced die with equal probabilities\n",
    "Sample_Space=[1,2,3,4,5,6]\n",
    "Prob_Values=[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "\n",
    "#Generating a sample\n",
    "weighted_choice(Sample_Space, Prob_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebf473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class for MDP environment definition that takes Transition Probability Matrix p(s'|s,a) and reward E[R_{t+1} | s,a,s'] as inputs\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possible states \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state, -0.01)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = random.choice(tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n",
    "        next_state = weighted_choice(possible_states, p=probs)\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state], dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                                              \"but is instead %s\" % (\n",
    "                                                              state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action], dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                                                          \"a dictionary but is instead %s\" % (\n",
    "                                                                              state, action,\n",
    "                                                                              type(transition_probs[state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                                    \"add up to %f (should be 1)\" % (state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                                                                 \"a dictionary but is instead %s\" % (\n",
    "                                                                 state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3c16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a MDP shown in this Figure https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\n",
    "\n",
    "transition_probs = {'s0':{'a0': {'s0': 0.5, 's2': 0.5},'a1': {'s2': 1}},\n",
    "                    's1':{'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "                    's2':{'a0': {'s0': 0.4, 's1': 0.6},'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}}\n",
    "                   }\n",
    "rewards = {'s1': {'a0': {'s0': +5}},'s2': {'a1': {'s0': -1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d69e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize an environment\n",
    "env=MDP(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d26571e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at s1\n"
     ]
    }
   ],
   "source": [
    "#To know the current state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f977e38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s0', 5, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To take an action and observe the next state, reward, 'whether terminal state reached or not', any other description.\n",
    "env.step('a0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc6005",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44cc8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating MDP given in Assignment 2\n",
    "\n",
    "transition_probability = {\n",
    "    's0':{'up':{'s0':0.9, 's1':0.1}, 'down':{'s4':0.8, 's0':0.1, 's1':0.1}, 'left':{'s0':0.9, 's4':0.1}, 'right':{'s1':0.8, 's0':0.1, 's4':0.1}},\n",
    "    's1':{'up':{'s1':0.8, 's0':0.1, 's2':0.1}, 'down':{'s1':0.8, 's0':0.1, 's2':0.1}, 'left':{'s0':0.8, 's1':0.2}, 'right':{'s2':0.8, 's1':0.2}},\n",
    "    's2':{'up':{'s2':0.8, 's1':0.1, 's3':0.1}, 'down':{'s6':0.8, 's1':0.1, 's3':0.1}, 'left':{'s1':0.8, 's2':0.1, 's6':0.1}, 'right':{'s3':0.8, 's2':0.1, 's6':0.1}},\n",
    "    's4':{'up':{'s0':0.8, 's4':0.2}, 'down':{'s8':0.8, 's4':0.2}, 'left':{'s4':0.8, 's0':0.1, 's8':0.1}, 'right':{'s4':0.8, 's0':0.1, 's8':0.1}},\n",
    "    's6':{'up':{'s2':0.8, 's6':0.1, 's7':0.1}, 'down':{'s10':0.8, 's6':0.1, 's7':0.1}, 'left':{'s6':0.8, 's2':0.1, 's10':0.1}, 'right':{'s7':0.8, 's2':0.1, 's10':0.1}},\n",
    "    's8':{'up':{'s4':0.8, 's8':0.1, 's9':0.1}, 'down':{'s8':0.9, 's9':0.1}, 'left':{'s8':0.9, 's4':0.1}, 'right':{'s9':0.8, 's8':0.1, 's4':0.1}},\n",
    "    's9':{'up':{'s9':0.8, 's8':0.1, 's10':0.1}, 'down':{'s9':0.8, 's8':0.1, 's10':0.1}, 'left':{'s8':0.8, 's9':0.2}, 'right':{'s10':0.8, 's9':0.2}},\n",
    "    's10':{'up':{'s6':0.8, 's9':0.1, 's11':0.1}, 'down':{'s10':0.8, 's9':0.1, 's11':0.1}, 'left':{'s9':0.8, 's10':0.1, 's6':0.1}, 'right':{'s11':0.8, 's10':0.1, 's6':0.1}},\n",
    "    's11':{'up':{'s7':0.8, 's11':0.1, 's10':0.1}, 'down':{'s11':0.9, 's10':0.1}, 'left':{'s10':0.8, 's11':0.1, 's7':0.1}, 'right':{'s11':0.9, 's7':0.1}}\n",
    "}\n",
    "reward = {'s2':{'right':{'s3':+1}, 'down':{'s3':+1}, 'up':{'s3':+1}}, 's6':{'right':{'s7':-1}, 'up':{'s7':-1}, 'down':{'s7':-1}}, 's11':{'right':{'s7':-1}, 'up':{'s7':-1}, 'left':{'s7':-1}}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59041713",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MDP(transition_probability, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3956fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s1': 0.8, 's2': 0.1, 's6': 0.1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_next_states('s2','left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61bfc241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.is_terminal('s4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148a5ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_transition_prob('s2','left','s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3379398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_reward('s6','right','s2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50f21d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s0': 0.8, 's1': 0.2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_next_states('s1','left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2667f210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s10'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e2aec74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at s10\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7815365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s11', -0.01, False, {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step('right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d92240",
   "metadata": {},
   "source": [
    "\n",
    "##### The states are considered to be in this form:\n",
    "\n",
    "[s0            s1.             s2.              s3]\n",
    "\n",
    "[s4.           s5.             s6.              s7]\n",
    "\n",
    "[s8.           s9.             s10              s11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a018a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.00001 #threshold\n",
    "gamma = 1 #Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203527aa",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12437218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of iterations to converge =  90 \n",
      "\n",
      "State Values: \n",
      " {'s0': 0.9597240654337827, 's1': 0.9737866045711732, 's2': 0.9862866319024044, 's3': 0, 's4': 0.9472240244231713, 's5': 0, 's6': 0.8965798684530988, 's7': 0, 's8': 0.9331614656973752, 's9': 0.920661404160804, 's10': 0.9068744893947835, 's11': 0.8067925049710254} \n",
      "\n",
      "State policy: \n",
      " {'s0': 'right', 's1': 'right', 's2': 'right', 's3': 'NA', 's4': 'up', 's5': 'NA', 's6': 'left', 's7': 'NA', 's8': 'up', 's9': 'left', 's10': 'left', 's11': 'down'}\n"
     ]
    }
   ],
   "source": [
    "states = ['s0','s1','s2','s4','s6','s8','s9','s10','s11']\n",
    "action = ['up','down','left','right']\n",
    "Vs = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0} #state values\n",
    "Vs_new = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0}\n",
    "diff_dict = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0} #diff values\n",
    "\n",
    "policy = {'s0':'NA','s1':'NA','s2':'NA','s3':'NA','s4':'NA','s5':'NA','s6':'NA','s7':'NA','s8':'NA','s9':'NA','s10':'NA'}\n",
    "action_val = {'up':0,'down':0,'left':0,'right':0}\n",
    "\n",
    "for t in range(100):\n",
    "    Vs_new = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0} #state values\n",
    "    m=0\n",
    "    for s in states:\n",
    "        for a in action:\n",
    "            nxtst = env.get_next_states(s,a) #next states\n",
    "            for s_ in nxtst:\n",
    "                action_val[a] += env.get_transition_prob(s,a,s_)*(env.get_reward(s,a,s_)+gamma*Vs[s_])\n",
    "        Vs_new[s] = action_val[max(zip(action_val.values(), action_val.keys()))[1]]\n",
    "        policy[s] = max(zip(action_val.values(), action_val.keys()))[1]\n",
    "        action_val = {'up':0,'down':0,'left':0,'right':0}\n",
    "\n",
    "    for key in Vs.keys():\n",
    "        diff_dict[key] = abs(Vs_new[key] - Vs.get(key, 0))\n",
    "    m = max(diff_dict.values())\n",
    "    \n",
    "    if m < delta:\n",
    "        print(\"No. of iterations to converge = \",t,\"\\n\")\n",
    "        break\n",
    "    Vs = Vs_new\n",
    "    \n",
    "print(\"State Values: \\n\",Vs_new,\"\\n\")\n",
    "print(\"State policy: \\n\",policy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a5197",
   "metadata": {},
   "source": [
    "### Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a70c3b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State values: \n",
      " {'s0': 0.9597242647056706, 's1': 0.9737867647057122, 's2': 0.9862867647057413, 's3': 0, 's4': 0.9472242647056272, 's5': 0, 's6': 0.8965808823518637, 's7': 0, 's8': 0.9331617647055648, 's9': 0.9206617647054994, 's10': 0.9068749999994575, 's11': 0.8068749965191249} \n",
      "\n",
      "Policy: \n",
      " {'s0': 'right', 's1': 'right', 's2': 'right', 's3': 'NA', 's4': 'up', 's5': 'NA', 's6': 'left', 's7': 'NA', 's8': 'up', 's9': 'left', 's10': 'left', 's11': 'down'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = ['s0','s1','s2','s4','s6','s8','s9','s10','s11']\n",
    "action = ['up','down','left','right']\n",
    "Vs = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0} #state values\n",
    "Vs_new = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0}\n",
    "action_val = {'up':0,'down':0,'left':0,'right':0}\n",
    "policy = {'s0':'right','s1':'right','s2':'right','s3':'NA','s4':'right','s5':'NA','s6':'right','s7':'NA','s8':'right','s9':'right','s10':'right','s11':'right'}\n",
    "policy_new = {'s0':'right','s1':'right','s2':'right','s3':'NA','s4':'right','s5':'NA','s6':'right','s7':'NA','s8':'right','s9':'right','s10':'right','s11':'right'}\n",
    "diff_dict = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0} #diff values\n",
    "action_val1 = 0\n",
    "\n",
    "for t in range(100):\n",
    "    value_converge = False\n",
    "    flag = 0\n",
    "    \n",
    "    #Policy Evaluation\n",
    "    for pe in range(100):\n",
    "        if value_converge == False:\n",
    "            Vs_new = {'s0':0,'s1':0,'s2':0,'s3':0,'s4':0,'s5':0,'s6':0,'s7':0,'s8':0,'s9':0,'s10':0,'s11':0}\n",
    "            m = 0\n",
    "            for s in states:\n",
    "                a = policy[s]\n",
    "                nxtst = env.get_next_states(s,policy[s]) #next states\n",
    "                for s_ in nxtst:\n",
    "                    action_val1 += env.get_transition_prob(s,a,s_)*(env.get_reward(s,a,s_)+gamma*Vs[s_])\n",
    "                Vs_new[s] = action_val1\n",
    "                action_val1 = 0\n",
    "            for key in Vs.keys():\n",
    "                diff_dict[key] = abs(Vs_new[key] - Vs[key])\n",
    "            m = max(diff_dict.values())\n",
    "            \n",
    "            if m < delta:\n",
    "                value_converge = True\n",
    "            Vs = Vs_new\n",
    "\n",
    "    #Policy Improvement\n",
    "    for s in states:\n",
    "        for a in action:\n",
    "            nxtst = env.get_next_states(s,a) #next states\n",
    "            for s_ in nxtst:\n",
    "                action_val[a] += env.get_transition_prob(s,a,s_)*(env.get_reward(s,a,s_)+gamma*Vs[s_])\n",
    "        policy_new[s] = max(zip(action_val.values(), action_val.keys()))[1]\n",
    "        action_val = {'up':0,'down':0,'left':0,'right':0}\n",
    "        \n",
    "    policy = policy_new\n",
    "\n",
    "print(\"State values: \\n\",Vs_new,\"\\n\")\n",
    "print(\"Policy: \\n\",policy_new,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab72bbc",
   "metadata": {},
   "source": [
    "#### \n",
    "Both Value iteration and Policy iteration are converging to same state values and policies. Policy iteration in general converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f5cad",
   "metadata": {},
   "source": [
    "##### Name: Prajeeth Babu Kodru\n",
    "##### Roll no: 22104071"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
